
data:
  raw_path: "data/raw/raw_data.csv"
  processed_path: "processed_data.csv" 
# Configuration du projet d'analyse de texte

# Répertoires de données
data:
  raw_path: "data/raw/new_dataset.csv"
  output_chunks_dir: "data/processed/train_clean_chunks"
  output_path: "data/processed/X_train_clean.csv"
  processed_path: "data/processed/x_train_clean.csv"

# input_file: data/raw/new_dataset.csv
# output_file: data/processed/x_train_clean.csv
# chunk_size: 10000


# Paramètres de prétraitement
preprocessing:
  remove_stopwords: true       # Supprimer les mots vides (stopwords)
  lowercase: true              # Convertir en minuscules
  lemmatize: false             # Lemmatization (non utilisé dans le script actuel)
  min_word_length: 3           # Longueur minimale d'un mot
  max_features: 10000          # Pour vectorisation (utilisé plus tard)

# Paramètres de modélisation
model:
  type: "tfidf"                # Options: "tfidf", "word2vec", "bert"
  vectorizer_params:
    ngram_range: [1, 2]
    max_df: 0.95
    min_df: 0.01

  bert:
    model_name: bert-base-uncased
    max_length: 128
    num_labels: 2
    epochs: 3
    batch_size: 16
        
# Paramètres d'entraînement
training:
  test_size: 0.2
  random_state: 42
model:
  model_filename: "model_v1_{timestamp}.pkl"  
  n_estimators: 100
  max_depth: 10
  ngram_range: [1, 2] 
  max_features: 5000
  param_grid:  
    tfidf__max_features: [3000, 5000]
    tfidf__ngram_range:
      - [1, 1]
      - [1, 2]
    clf__n_estimators: [100, 200]
    clf__max_depth: [null, 20, 50]
    clf__min_samples_split: [2, 5]
mlflow:
  tracking_uri: "http://localhost:5000"
  experiment_name: "random_forest_text_classification"
  log_model: true
  log_metrics: true
  log_artifacts: true
  artifact_location: "mlruns"