# Configuration générale du projet d'analyse de texte

# Répertoires de données
data:
  raw_path: "data/raw/new_dataset.csv"        # Chemin vers les données brutes
  processed_path: "data/processed/new_dataset.csv"  # Chemin vers les données traitées
  output_chunks_dir: "data/processed/train_clean_chunks"  # Dossier pour stocker les chunks traités
  output_path: "data/processed/X_train_clean.csv"  # Chemin vers les données d'entraînement nettoyées
  test_size: 0.2  # Proportion des données pour le test (80% pour l'entraînement)
  random_state: 42  # Seed pour la reproductibilité des résultats
  chunk_size: 1000 

# Paramètres de prétraitement des données
preprocessing:
  remove_stopwords: true        # Supprimer les mots vides (stopwords)
  lowercase: true               # Convertir en minuscules
  lemmatize: false              # Lemmatization (non utilisé dans le script actuel)
  min_word_length: 3            # Longueur minimale d'un mot
  max_features: 10000           # Nombre maximal de caractéristiques pour la vectorisation

# Configuration des modèles
model:
  # Configuration du modèle Random Forest
  random_forest:
    output_dir: "models/random_forest_v1"  # Dossier pour sauvegarder le modèle Random Forest
    type: "tfidf"                # Type de vectorisation : "tfidf", "word2vec", "bert"
    vectorizer_params:
      ngram_range: [1, 2]        # Plage des n-grammes (uniques et bigrammes)
      max_df: 0.95               # Ignorer les mots apparaissant dans plus de 95% des documents
      min_df: 0.01               # Garder les mots qui apparaissent dans au moins 1% des documents
    vectorizer_path: "models/random_forest_model.pkl"  # Chemin pour sauvegarder le modèle TF-IDF
    model_filename: "random_forest_model.pkl"  # Nom du fichier du modèle Random Forest
    n_estimators: 100           # Nombre d'arbres dans la forêt
    max_depth: 10               # Profondeur maximale des arbres
    param_grid:
      tfidf__max_features: [3000, 5000]
      tfidf__ngram_range:
        - [1, 1]
        - [1, 2]
      clf__n_estimators: [100, 200]
      clf__max_depth: [null, 20, 50]
      clf__min_samples_split: [2, 5]
  
  # Configuration du modèle BERT
  bert:
    bert_path: 'models/bert_v1'  # Dossier contenant le modèle BERT pré-entraîné
    model_name: "bert-base-uncased"  # Nom du modèle BERT à utiliser
    max_length: 128              # Longueur maximale des séquences d'entrée
    num_labels: 2                # Nombre de labels pour la classification (2 pour binaire)
    epochs: 3                    # Nombre d'époques d'entraînement
    batch_size: 16               # Taille des lots (batch size)
    warmup_steps: 0           # Pourcentage des étapes d'entraînement pour le warm-up
    weight_decay: 0.1            # Taux de décroissance du poids pour la régularisation
    save_total_limit: 1          # Nombre maximal de modèles sauvegardés
    logging_steps: 100           # Fréquence des logs pendant l'entraînement
    output_dir: "models/bert_output_dir"  # Dossier où le modèle sera sauvegardé
    test_size: 0.2 
    random_state: 42  # Seed pour la reproductibilité des résultats
    

# Paramètres d'entraînement pour les modèles
training:
  test_size: 0.2                # Proportion des données pour l'évaluation
  random_state: 42              # Seed pour la reproductibilité des résultats

# Configuration de MLflow pour le suivi des expériences
mlflow:
  tracking_uri: "http://localhost:5000"  # URI du serveur MLflow
  experiment_name: "random_forest_text_classification"  # Nom de l'expérience
  log_model: true                          # Enregistrer le modèle dans MLflow
  log_metrics: true                        # Enregistrer les métriques dans MLflow
  log_artifacts: true                      # Enregistrer les artefacts dans MLflow
  artifact_location: "mlruns"              # Répertoire des artefacts sauvegardés par MLflow
