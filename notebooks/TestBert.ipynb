{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6db441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 50000 lignes aléatoires écrites dans ../data/raw/new_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "\n",
    "def reservoir_sample(file_path, sample_size):\n",
    "    \"\"\"Sélectionne aléatoirement sample_size lignes d'un gros fichier CSV.\"\"\"\n",
    "    reservoir = []\n",
    "    with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        header = next(reader)  # Sauvegarder l'en-tête\n",
    "        for i, row in enumerate(reader):\n",
    "            if i < sample_size:\n",
    "                reservoir.append(row)\n",
    "            else:\n",
    "                j = random.randint(0, i)\n",
    "                if j < sample_size:\n",
    "                    reservoir[j] = row\n",
    "    return header, reservoir\n",
    "\n",
    "# Paramètres\n",
    "input_file = '../data/raw/train.csv'\n",
    "output_file = '../data/raw/new_dataset.csv'\n",
    "sample_size = 50_000\n",
    "\n",
    "# Exécution\n",
    "header, sample_rows = reservoir_sample(input_file, sample_size)\n",
    "\n",
    "# Écriture dans le nouveau fichier\n",
    "with open(output_file, mode='w', newline='', encoding='utf-8') as f_out:\n",
    "    writer = csv.writer(f_out)\n",
    "    writer.writerow(header)\n",
    "    writer.writerows(sample_rows)\n",
    "\n",
    "print(f\"✅ {sample_size} lignes aléatoires écrites dans {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaa5cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installer les dépendances en utilisant un miroir PyPI pour éviter les erreurs réseau\n",
    "!pip install pyyaml accelerate>=0.26.0 transformers[torch] --index-url https://mirrors.aliyun.com/pypi/simple/\n",
    "\n",
    "# Importer les bibliothèques\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import yaml\n",
    "import logging\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Configurer le logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),\n",
    "        logging.FileHandler(\"/content/drive/MyDrive/MLOps/output.log\")\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Vérifier le GPU\n",
    "logger.info(f\"GPU disponible : {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    logger.info(f\"Nom du GPU : {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Tester la connectivité réseau\n",
    "logger.info(\"Test de la connectivité réseau\")\n",
    "!ping pypi.org\n",
    "\n",
    "# Définir les classes\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, config):\n",
    "        self.raw_path = config['data']['raw_path']\n",
    "        self.processed_path = config['data']['processed_path']\n",
    "        self.chunk_size = config.get('data', {}).get('chunk_size', 100000)\n",
    "\n",
    "    def load_raw_data(self):\n",
    "        logger.info(\"Chargement des données brutes\")\n",
    "        df = pd.read_csv(self.raw_path)\n",
    "        return df\n",
    "\n",
    "    def preprocess_chunk(self, df_chunk):\n",
    "        logger.info(\"Prétraitement d'un chunk de données\")\n",
    "        df_chunk = df_chunk.dropna(subset=['text', 'label'])\n",
    "        df_chunk['text'] = df_chunk['text'].astype(str)\n",
    "        # Ajuster les labels (0 et 1 au lieu de 1 et 2)\n",
    "        if df_chunk['label'].min() == 1:\n",
    "            logger.info(\"Ajustement des labels de 1,2 à 0,1\")\n",
    "            df_chunk['label'] = df_chunk['label'] - 1\n",
    "        return df_chunk\n",
    "\n",
    "    def process_and_save_chunks(self): \n",
    "        logger.info(\"Début du prétraitement des données\")\n",
    "        if os.path.exists(self.processed_path):\n",
    "            logger.info(\"Fichier traité existe déjà\")\n",
    "            return\n",
    "        chunks = pd.read_csv(self.raw_path, chunksize=self.chunk_size)\n",
    "        processed_chunks = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            logger.info(f\"Traitement du chunk {i+1}\")\n",
    "            processed_chunk = self.preprocess_chunk(chunk)\n",
    "            processed_chunks.append(processed_chunk)\n",
    "        processed_df = pd.concat(processed_chunks, ignore_index=True)\n",
    "        os.makedirs(os.path.dirname(self.processed_path), exist_ok=True)\n",
    "        processed_df.to_csv(self.processed_path, index=False)\n",
    "        logger.info(\"Prétraitement terminé, fichier sauvegardé\")\n",
    "\n",
    "class BertTextClassifier:\n",
    "    def __init__(self, config_path):\n",
    "        with open(config_path, 'r') as file:\n",
    "            self.config = yaml.safe_load(file)\n",
    "        self.model_name = self.config['model']['bert']['model_name']\n",
    "        self.num_labels = self.config['model']['bert']['num_labels']\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = BertForSequenceClassification.from_pretrained(\n",
    "            self.model_name,\n",
    "            num_labels=self.num_labels\n",
    "        )\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        logger.info(f\"Modèle BERT chargé sur {self.device}\")\n",
    "\n",
    "    def train(self, df):\n",
    "        logger.info(f\"Entraînement BERT sur {len(df)} lignes\")\n",
    "        # Ajuster les labels si nécessaire\n",
    "        labels = df['label'].values\n",
    "        if labels.min() == 1:\n",
    "            logger.info(\"Ajustement des labels de 1,2 à 0,1\")\n",
    "            labels = labels - 1\n",
    "        train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "            df['text'].values, labels, test_size=0.2, random_state=42\n",
    "        )\n",
    "        train_dataset = SentimentDataset(train_texts, train_labels, self.tokenizer)\n",
    "        val_dataset = SentimentDataset(val_texts, val_labels, self.tokenizer)\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir='/content/drive/MyDrive/MLOps/results',\n",
    "            num_train_epochs=self.config['model']['bert']['epochs'],\n",
    "            per_device_train_batch_size=self.config['model']['bert']['batch_size'],\n",
    "            per_device_eval_batch_size=self.config['model']['bert']['batch_size'],\n",
    "            warmup_steps=500,\n",
    "            weight_decay=0.01,\n",
    "            logging_dir='/content/drive/MyDrive/MLOps/logs',\n",
    "            logging_steps=10,\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True,\n",
    "            dataloader_pin_memory=torch.cuda.is_available(),\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            compute_metrics=lambda p: {'accuracy': (p.predictions.argmax(-1) == p.label_ids).mean()}\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        logger.info(\"Entraînement BERT terminé\")\n",
    "\n",
    "    def save_model(self, output_dir):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        self.model.save_pretrained(output_dir)\n",
    "        self.tokenizer.save_pretrained(output_dir)\n",
    "        logger.info(f\"Modèle BERT sauvegardé à {output_dir}\")\n",
    "\n",
    "# Vérifier les données\n",
    "logger.info(\"Vérification du chemin des données\")\n",
    "# !ls /kaggle/input/your-dataset\n",
    "# logger.info(\"Vérification des premières lignes\")\n",
    "# !head /kaggle/input/your-dataset/raw.csv\n",
    "\n",
    "# Définir la configuration\n",
    "config = {\n",
    "    'data': {\n",
    "        'raw_path': data_path,  # Remplacer par le chemin réel\n",
    "        'processed_path': '/content/drive/MyDrive/MLOps/data/processed.csv',\n",
    "        'chunk_size': 100000\n",
    "    },\n",
    "    'model': {\n",
    "        'bert': {\n",
    "            'model_name': 'bert-base-uncased',\n",
    "            'num_labels': 2,\n",
    "            'epochs': 3,\n",
    "            'batch_size': 16\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Sauvegarder la configuration\n",
    "os.makedirs('/content/drive/MyDrive/MLOps/config', exist_ok=True)\n",
    "with open('/content/drive/MyDrive/MLOps/config/config.yaml', 'w') as file:\n",
    "    yaml.safe_dump(config, file)\n",
    "\n",
    "# Exécuter l'entraînement\n",
    "try:\n",
    "    logger.info(\"Début du prétraitement des données\")\n",
    "    data_loader = DataLoader(config)\n",
    "    data_loader.process_and_save_chunks()\n",
    "\n",
    "    logger.info(\"Début de l'entraînement BERT\")\n",
    "    bert_classifier = BertTextClassifier('/content/drive/MyDrive/MLOps/config/config.yaml')\n",
    "    df = pd.read_csv(data_loader.processed_path).sample(n=10000, random_state=42)\n",
    "    bert_classifier.train(df)\n",
    "    bert_classifier.save_model('/content/drive/MyDrive/MLOps/models/bert_v1')\n",
    "    logger.info(\"Entraînement terminé\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Erreur pendant l'exécution : {str(e)}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c839b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"_distutils_hack\")\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))\n",
    "\n",
    "import yaml\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import emoji\n",
    "\n",
    "# Configurer le logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Définir les classes\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class TextClassificationModel:\n",
    "    def __init__(self, config_path: str):\n",
    "        with open(config_path, 'r') as file:\n",
    "            self.config = yaml.safe_load(file)\n",
    "        self.data_loader = DataLoader(config_path)\n",
    "    \n",
    "    def save_model(self, model, path: str):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def train(self, df: pd.DataFrame):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class BertTextClassifier(TextClassificationModel):\n",
    "    def __init__(self, config_path: str):\n",
    "        super().__init__(config_path)\n",
    "        self.model_name = self.config['model']['bert']['model_name']\n",
    "        self.num_labels = self.config['model']['bert']['num_labels']\n",
    "        self.max_length = self.config['model']['bert']['max_length']\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = BertForSequenceClassification.from_pretrained(\n",
    "            self.model_name,\n",
    "            num_labels=self.num_labels\n",
    "        )\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        logger.info(f\"Modèle BERT chargé ({self.model_name}) sur {self.device}\")\n",
    "\n",
    "    def preprocess_data(self, df: pd.DataFrame):\n",
    "        \"\"\"Prétraitement spécifique pour BERT.\"\"\"\n",
    "        logger.info(f\"Prétraitement de {len(df)} lignes\")\n",
    "        \n",
    "        # Nettoyage\n",
    "        df = df.dropna(subset=['text', 'label'])\n",
    "        df = df[df['text'].str.strip() != '']\n",
    "        df = df[df['text'].str.split().str.len() >= 3]  # Minimum 3 mots\n",
    "        df['text'] = df['text'].apply(lambda x: re.sub(r'\\s+', ' ', x.strip()))\n",
    "\n",
    "        # Ajustement des labels (de 1,2 à 0,1)\n",
    "        if df['label'].min() == 1:\n",
    "            logger.info(\"Ajustement des labels de 1,2 à 0,1\")\n",
    "            df['label'] = df['label'] - 1\n",
    "\n",
    "        # Analyse des longueurs\n",
    "        lengths = [len(self.tokenizer.encode(text, add_special_tokens=True)) for text in df['text']]\n",
    "        logger.info(f\"Longueur moyenne : {np.mean(lengths)}, Max : {np.max(lengths)}, 90e percentile : {np.percentile(lengths, 90)}\")\n",
    "        mlflow.log_metric(\"mean_text_length\", np.mean(lengths))\n",
    "        mlflow.log_metric(\"max_text_length\", np.max(lengths))\n",
    "\n",
    "        # Distribution des labels\n",
    "        label_dist = df['label'].value_counts().to_dict()\n",
    "        logger.info(f\"Distribution des labels : {label_dist}\")\n",
    "        mlflow.log_metric(\"label_0_count\", label_dist.get(0, 0))\n",
    "        mlflow.log_metric(\"label_1_count\", label_dist.get(1, 0))\n",
    "\n",
    "        return df\n",
    "\n",
    "    @timer_decorator\n",
    "    def train(self, df: pd.DataFrame):\n",
    "        logger.info(f\"Entraînement BERT sur {len(df)} lignes\")\n",
    "        \n",
    "        # Prétraitement\n",
    "        df = self.preprocess_data(df)\n",
    "        \n",
    "        # Calcul des poids des classes\n",
    "        class_weights = compute_class_weight('balanced', classes=np.array([0, 1]), y=df['label'])\n",
    "        class_weights = torch.tensor(class_weights, dtype=torch.float).to(self.device)\n",
    "        logger.info(f\"Poids des classes : {class_weights.tolist()}\")\n",
    "\n",
    "        # Séparation train/test\n",
    "        train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "            df['text'].values, df['label'].values, test_size=0.2, random_state=42\n",
    "        )\n",
    "        train_dataset = SentimentDataset(train_texts, train_labels, self.tokenizer, self.max_length)\n",
    "        val_dataset = SentimentDataset(val_texts, val_labels, self.tokenizer, self.max_length)\n",
    "\n",
    "        # Configuration de l'entraînement\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.config['training']['output_dir'],\n",
    "            num_train_epochs=self.config['model']['bert']['epochs'],\n",
    "            per_device_train_batch_size=self.config['model']['bert']['batch_size'],\n",
    "            per_device_eval_batch_size=self.config['model']['bert']['batch_size'],\n",
    "            warmup_steps=500,\n",
    "            weight_decay=0.01,\n",
    "            logging_dir=self.config['training']['logging_dir'],\n",
    "            logging_steps=10,\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True,\n",
    "            dataloader_pin_memory=torch.cuda.is_available(),\n",
    "            fp16=True,  # Précision mixte pour optimiser la mémoire\n",
    "        )\n",
    "\n",
    "        # Trainer personnalisé pour la pondération des pertes\n",
    "        class CustomTrainer(Trainer):\n",
    "            def compute_loss(self, model, inputs, return_outputs=False):\n",
    "                labels = inputs.pop('labels')\n",
    "                outputs = model(**inputs)\n",
    "                logits = outputs.logits\n",
    "                loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "                loss = loss_fct(logits, labels)\n",
    "                return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "        trainer = CustomTrainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            compute_metrics=lambda p: {'accuracy': (p.predictions.argmax(-1) == p.label_ids).mean()}\n",
    "        )\n",
    "\n",
    "        # MLflow\n",
    "        experiment_name = \"TextClassificationExperiment\"\n",
    "        experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "        if experiment is None:\n",
    "            experiment_id = mlflow.create_experiment(experiment_name)\n",
    "        else:\n",
    "            experiment_id = experiment.experiment_id\n",
    "\n",
    "        with mlflow.start_run(experiment_id=experiment_id):\n",
    "            mlflow.log_param(\"model_type\", \"BERT\")\n",
    "            mlflow.log_param(\"model_name\", self.model_name)\n",
    "            mlflow.log_param(\"epochs\", self.config['model']['bert']['epochs'])\n",
    "            mlflow.log_param(\"batch_size\", self.config['model']['bert']['batch_size'])\n",
    "            mlflow.log_param(\"max_length\", self.max_length)\n",
    "\n",
    "            # Entraînement\n",
    "            trainer.train()\n",
    "\n",
    "            # Évaluation\n",
    "            predictions = trainer.predict(val_dataset).predictions.argmax(-1)\n",
    "            accuracy = accuracy_score(val_labels, predictions)\n",
    "            mlflow.log_metric(\"val_accuracy\", accuracy)\n",
    "            mlflow.pytorch.log_model(self.model, \"bert_model\")\n",
    "            logger.info(f\"Précision sur validation : {accuracy:.4f}\")\n",
    "\n",
    "    def save_model(self, output_dir: str):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        self.model.save_pretrained(output_dir)\n",
    "        self.tokenizer.save_pretrained(output_dir)\n",
    "        logger.info(f\"Modèle BERT sauvegardé à {output_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config_path = os.path.join(os.path.dirname(__file__), \"../../config/config.yaml\")\n",
    "    config = {\n",
    "        'data': {\n",
    "            'raw_path': '/path/to/raw.csv',\n",
    "            'processed_path': '/content/drive/MyDrive/MLOps/data/processed.parquet',\n",
    "            'chunk_size': 100000\n",
    "        },\n",
    "        'model': {\n",
    "            'bert': {\n",
    "                'model_name': 'bert-base-uncased',\n",
    "                'num_labels': 2,\n",
    "                'epochs': 3,\n",
    "                'batch_size': 16,\n",
    "                'max_length': 128\n",
    "            }\n",
    "        },\n",
    "        'training': {\n",
    "            'output_dir': '/content/drive/MyDrive/MLOps/results',\n",
    "            'logging_dir': '/content/drive/MyDrive/MLOps/logs'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Sauvegarder la configuration\n",
    "    os.makedirs(os.path.dirname(config_path), exist_ok=True)\n",
    "    with open(config_path, 'w') as file:\n",
    "        yaml.safe_dump(config, file)\n",
    "\n",
    "    try:\n",
    "        logger.info(\"Début du prétraitement des données\")\n",
    "        data_loader = DataLoader(config_path)\n",
    "        data_loader.process_and_save_chunks()\n",
    "\n",
    "        logger.info(\"Début de l'entraînement BERT\")\n",
    "        bert_classifier = BertTextClassifier(config_path)\n",
    "        df = pd.read_parquet(data_loader.processed_path)  # Charger toutes les 100 000 lignes\n",
    "        bert_classifier.train(df)\n",
    "        bert_classifier.save_model('/content/drive/MyDrive/MLOps/models/bert_v1')\n",
    "        logger.info(\"Entraînement terminé\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur pendant l'exécution : {str(e)}\", exc_info=True)\n",
    "        raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
