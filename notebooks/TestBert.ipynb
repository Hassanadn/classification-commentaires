{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6db441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 50000 lignes aléatoires écrites dans ../data/raw/new_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "\n",
    "def reservoir_sample(file_path, sample_size):\n",
    "    \"\"\"Sélectionne aléatoirement sample_size lignes d'un gros fichier CSV.\"\"\"\n",
    "    reservoir = []\n",
    "    with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        header = next(reader)  # Sauvegarder l'en-tête\n",
    "        for i, row in enumerate(reader):\n",
    "            if i < sample_size:\n",
    "                reservoir.append(row)\n",
    "            else:\n",
    "                j = random.randint(0, i)\n",
    "                if j < sample_size:\n",
    "                    reservoir[j] = row\n",
    "    return header, reservoir\n",
    "\n",
    "# Paramètres\n",
    "input_file = '../data/raw/train.csv'\n",
    "output_file = '../data/raw/new_dataset.csv'\n",
    "sample_size = 50_000\n",
    "\n",
    "# Exécution\n",
    "header, sample_rows = reservoir_sample(input_file, sample_size)\n",
    "\n",
    "# Écriture dans le nouveau fichier\n",
    "with open(output_file, mode='w', newline='', encoding='utf-8') as f_out:\n",
    "    writer = csv.writer(f_out)\n",
    "    writer.writerow(header)\n",
    "    writer.writerows(sample_rows)\n",
    "\n",
    "print(f\"✅ {sample_size} lignes aléatoires écrites dans {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaa5cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installer les dépendances en utilisant un miroir PyPI pour éviter les erreurs réseau\n",
    "!pip install pyyaml accelerate>=0.26.0 transformers[torch] --index-url https://mirrors.aliyun.com/pypi/simple/\n",
    "\n",
    "# Importer les bibliothèques\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import yaml\n",
    "import logging\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Configurer le logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),\n",
    "        logging.FileHandler(\"/content/drive/MyDrive/MLOps/output.log\")\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Vérifier le GPU\n",
    "logger.info(f\"GPU disponible : {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    logger.info(f\"Nom du GPU : {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Tester la connectivité réseau\n",
    "logger.info(\"Test de la connectivité réseau\")\n",
    "!ping pypi.org\n",
    "\n",
    "# Définir les classes\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, config):\n",
    "        self.raw_path = config['data']['raw_path']\n",
    "        self.processed_path = config['data']['processed_path']\n",
    "        self.chunk_size = config.get('data', {}).get('chunk_size', 100000)\n",
    "\n",
    "    def load_raw_data(self):\n",
    "        logger.info(\"Chargement des données brutes\")\n",
    "        df = pd.read_csv(self.raw_path)\n",
    "        return df\n",
    "\n",
    "    def preprocess_chunk(self, df_chunk):\n",
    "        logger.info(\"Prétraitement d'un chunk de données\")\n",
    "        df_chunk = df_chunk.dropna(subset=['text', 'label'])\n",
    "        df_chunk['text'] = df_chunk['text'].astype(str)\n",
    "        # Ajuster les labels (0 et 1 au lieu de 1 et 2)\n",
    "        if df_chunk['label'].min() == 1:\n",
    "            logger.info(\"Ajustement des labels de 1,2 à 0,1\")\n",
    "            df_chunk['label'] = df_chunk['label'] - 1\n",
    "        return df_chunk\n",
    "\n",
    "    def process_and_save_chunks(self): \n",
    "        logger.info(\"Début du prétraitement des données\")\n",
    "        if os.path.exists(self.processed_path):\n",
    "            logger.info(\"Fichier traité existe déjà\")\n",
    "            return\n",
    "        chunks = pd.read_csv(self.raw_path, chunksize=self.chunk_size)\n",
    "        processed_chunks = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            logger.info(f\"Traitement du chunk {i+1}\")\n",
    "            processed_chunk = self.preprocess_chunk(chunk)\n",
    "            processed_chunks.append(processed_chunk)\n",
    "        processed_df = pd.concat(processed_chunks, ignore_index=True)\n",
    "        os.makedirs(os.path.dirname(self.processed_path), exist_ok=True)\n",
    "        processed_df.to_csv(self.processed_path, index=False)\n",
    "        logger.info(\"Prétraitement terminé, fichier sauvegardé\")\n",
    "\n",
    "class BertTextClassifier:\n",
    "    def __init__(self, config_path):\n",
    "        with open(config_path, 'r') as file:\n",
    "            self.config = yaml.safe_load(file)\n",
    "        self.model_name = self.config['model']['bert']['model_name']\n",
    "        self.num_labels = self.config['model']['bert']['num_labels']\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = BertForSequenceClassification.from_pretrained(\n",
    "            self.model_name,\n",
    "            num_labels=self.num_labels\n",
    "        )\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        logger.info(f\"Modèle BERT chargé sur {self.device}\")\n",
    "\n",
    "    def train(self, df):\n",
    "        logger.info(f\"Entraînement BERT sur {len(df)} lignes\")\n",
    "        # Ajuster les labels si nécessaire\n",
    "        labels = df['label'].values\n",
    "        if labels.min() == 1:\n",
    "            logger.info(\"Ajustement des labels de 1,2 à 0,1\")\n",
    "            labels = labels - 1\n",
    "        train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "            df['text'].values, labels, test_size=0.2, random_state=42\n",
    "        )\n",
    "        train_dataset = SentimentDataset(train_texts, train_labels, self.tokenizer)\n",
    "        val_dataset = SentimentDataset(val_texts, val_labels, self.tokenizer)\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir='/content/drive/MyDrive/MLOps/results',\n",
    "            num_train_epochs=self.config['model']['bert']['epochs'],\n",
    "            per_device_train_batch_size=self.config['model']['bert']['batch_size'],\n",
    "            per_device_eval_batch_size=self.config['model']['bert']['batch_size'],\n",
    "            warmup_steps=500,\n",
    "            weight_decay=0.01,\n",
    "            logging_dir='/content/drive/MyDrive/MLOps/logs',\n",
    "            logging_steps=10,\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True,\n",
    "            dataloader_pin_memory=torch.cuda.is_available(),\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            compute_metrics=lambda p: {'accuracy': (p.predictions.argmax(-1) == p.label_ids).mean()}\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        logger.info(\"Entraînement BERT terminé\")\n",
    "\n",
    "    def save_model(self, output_dir):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        self.model.save_pretrained(output_dir)\n",
    "        self.tokenizer.save_pretrained(output_dir)\n",
    "        logger.info(f\"Modèle BERT sauvegardé à {output_dir}\")\n",
    "\n",
    "# Vérifier les données\n",
    "logger.info(\"Vérification du chemin des données\")\n",
    "# !ls /kaggle/input/your-dataset\n",
    "# logger.info(\"Vérification des premières lignes\")\n",
    "# !head /kaggle/input/your-dataset/raw.csv\n",
    "\n",
    "# Définir la configuration\n",
    "config = {\n",
    "    'data': {\n",
    "        'raw_path': data_path,  # Remplacer par le chemin réel\n",
    "        'processed_path': '/content/drive/MyDrive/MLOps/data/processed.csv',\n",
    "        'chunk_size': 100000\n",
    "    },\n",
    "    'model': {\n",
    "        'bert': {\n",
    "            'model_name': 'bert-base-uncased',\n",
    "            'num_labels': 2,\n",
    "            'epochs': 3,\n",
    "            'batch_size': 16\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Sauvegarder la configuration\n",
    "os.makedirs('/content/drive/MyDrive/MLOps/config', exist_ok=True)\n",
    "with open('/content/drive/MyDrive/MLOps/config/config.yaml', 'w') as file:\n",
    "    yaml.safe_dump(config, file)\n",
    "\n",
    "# Exécuter l'entraînement\n",
    "try:\n",
    "    logger.info(\"Début du prétraitement des données\")\n",
    "    data_loader = DataLoader(config)\n",
    "    data_loader.process_and_save_chunks()\n",
    "\n",
    "    logger.info(\"Début de l'entraînement BERT\")\n",
    "    bert_classifier = BertTextClassifier('/content/drive/MyDrive/MLOps/config/config.yaml')\n",
    "    df = pd.read_csv(data_loader.processed_path).sample(n=10000, random_state=42)\n",
    "    bert_classifier.train(df)\n",
    "    bert_classifier.save_model('/content/drive/MyDrive/MLOps/models/bert_v1')\n",
    "    logger.info(\"Entraînement terminé\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Erreur pendant l'exécution : {str(e)}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c839b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"_distutils_hack\")\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))\n",
    "\n",
    "import yaml\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import emoji\n",
    "\n",
    "# Configurer le logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Définir les classes\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class TextClassificationModel:\n",
    "    def __init__(self, config_path: str):\n",
    "        with open(config_path, 'r') as file:\n",
    "            self.config = yaml.safe_load(file)\n",
    "        self.data_loader = DataLoader(config_path)\n",
    "    \n",
    "    def save_model(self, model, path: str):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def train(self, df: pd.DataFrame):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class BertTextClassifier(TextClassificationModel):\n",
    "    def __init__(self, config_path: str):\n",
    "        super().__init__(config_path)\n",
    "        self.model_name = self.config['model']['bert']['model_name']\n",
    "        self.num_labels = self.config['model']['bert']['num_labels']\n",
    "        self.max_length = self.config['model']['bert']['max_length']\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = BertForSequenceClassification.from_pretrained(\n",
    "            self.model_name,\n",
    "            num_labels=self.num_labels\n",
    "        )\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        logger.info(f\"Modèle BERT chargé ({self.model_name}) sur {self.device}\")\n",
    "\n",
    "    def preprocess_data(self, df: pd.DataFrame):\n",
    "        \"\"\"Prétraitement spécifique pour BERT.\"\"\"\n",
    "        logger.info(f\"Prétraitement de {len(df)} lignes\")\n",
    "        \n",
    "        # Nettoyage\n",
    "        df = df.dropna(subset=['text', 'label'])\n",
    "        df = df[df['text'].str.strip() != '']\n",
    "        df = df[df['text'].str.split().str.len() >= 3]  # Minimum 3 mots\n",
    "        df['text'] = df['text'].apply(lambda x: re.sub(r'\\s+', ' ', x.strip()))\n",
    "\n",
    "        # Ajustement des labels (de 1,2 à 0,1)\n",
    "        if df['label'].min() == 1:\n",
    "            logger.info(\"Ajustement des labels de 1,2 à 0,1\")\n",
    "            df['label'] = df['label'] - 1\n",
    "\n",
    "        # Analyse des longueurs\n",
    "        lengths = [len(self.tokenizer.encode(text, add_special_tokens=True)) for text in df['text']]\n",
    "        logger.info(f\"Longueur moyenne : {np.mean(lengths)}, Max : {np.max(lengths)}, 90e percentile : {np.percentile(lengths, 90)}\")\n",
    "        mlflow.log_metric(\"mean_text_length\", np.mean(lengths))\n",
    "        mlflow.log_metric(\"max_text_length\", np.max(lengths))\n",
    "\n",
    "        # Distribution des labels\n",
    "        label_dist = df['label'].value_counts().to_dict()\n",
    "        logger.info(f\"Distribution des labels : {label_dist}\")\n",
    "        mlflow.log_metric(\"label_0_count\", label_dist.get(0, 0))\n",
    "        mlflow.log_metric(\"label_1_count\", label_dist.get(1, 0))\n",
    "\n",
    "        return df\n",
    "\n",
    "    @timer_decorator\n",
    "    def train(self, df: pd.DataFrame):\n",
    "        logger.info(f\"Entraînement BERT sur {len(df)} lignes\")\n",
    "        \n",
    "        # Prétraitement\n",
    "        df = self.preprocess_data(df)\n",
    "        \n",
    "        # Calcul des poids des classes\n",
    "        class_weights = compute_class_weight('balanced', classes=np.array([0, 1]), y=df['label'])\n",
    "        class_weights = torch.tensor(class_weights, dtype=torch.float).to(self.device)\n",
    "        logger.info(f\"Poids des classes : {class_weights.tolist()}\")\n",
    "\n",
    "        # Séparation train/test\n",
    "        train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "            df['text'].values, df['label'].values, test_size=0.2, random_state=42\n",
    "        )\n",
    "        train_dataset = SentimentDataset(train_texts, train_labels, self.tokenizer, self.max_length)\n",
    "        val_dataset = SentimentDataset(val_texts, val_labels, self.tokenizer, self.max_length)\n",
    "\n",
    "        # Configuration de l'entraînement\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.config['training']['output_dir'],\n",
    "            num_train_epochs=self.config['model']['bert']['epochs'],\n",
    "            per_device_train_batch_size=self.config['model']['bert']['batch_size'],\n",
    "            per_device_eval_batch_size=self.config['model']['bert']['batch_size'],\n",
    "            warmup_steps=500,\n",
    "            weight_decay=0.01,\n",
    "            logging_dir=self.config['training']['logging_dir'],\n",
    "            logging_steps=10,\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True,\n",
    "            dataloader_pin_memory=torch.cuda.is_available(),\n",
    "            fp16=True,  # Précision mixte pour optimiser la mémoire\n",
    "        )\n",
    "\n",
    "        # Trainer personnalisé pour la pondération des pertes\n",
    "        class CustomTrainer(Trainer):\n",
    "            def compute_loss(self, model, inputs, return_outputs=False):\n",
    "                labels = inputs.pop('labels')\n",
    "                outputs = model(**inputs)\n",
    "                logits = outputs.logits\n",
    "                loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "                loss = loss_fct(logits, labels)\n",
    "                return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "        trainer = CustomTrainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            compute_metrics=lambda p: {'accuracy': (p.predictions.argmax(-1) == p.label_ids).mean()}\n",
    "        )\n",
    "\n",
    "        # MLflow\n",
    "        experiment_name = \"TextClassificationExperiment\"\n",
    "        experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "        if experiment is None:\n",
    "            experiment_id = mlflow.create_experiment(experiment_name)\n",
    "        else:\n",
    "            experiment_id = experiment.experiment_id\n",
    "\n",
    "        with mlflow.start_run(experiment_id=experiment_id):\n",
    "            mlflow.log_param(\"model_type\", \"BERT\")\n",
    "            mlflow.log_param(\"model_name\", self.model_name)\n",
    "            mlflow.log_param(\"epochs\", self.config['model']['bert']['epochs'])\n",
    "            mlflow.log_param(\"batch_size\", self.config['model']['bert']['batch_size'])\n",
    "            mlflow.log_param(\"max_length\", self.max_length)\n",
    "\n",
    "            # Entraînement\n",
    "            trainer.train()\n",
    "\n",
    "            # Évaluation\n",
    "            predictions = trainer.predict(val_dataset).predictions.argmax(-1)\n",
    "            accuracy = accuracy_score(val_labels, predictions)\n",
    "            mlflow.log_metric(\"val_accuracy\", accuracy)\n",
    "            mlflow.pytorch.log_model(self.model, \"bert_model\")\n",
    "            logger.info(f\"Précision sur validation : {accuracy:.4f}\")\n",
    "\n",
    "    def save_model(self, output_dir: str):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        self.model.save_pretrained(output_dir)\n",
    "        self.tokenizer.save_pretrained(output_dir)\n",
    "        logger.info(f\"Modèle BERT sauvegardé à {output_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config_path = os.path.join(os.path.dirname(__file__), \"../../config/config.yaml\")\n",
    "    config = {\n",
    "        'data': {\n",
    "            'raw_path': '/path/to/raw.csv',\n",
    "            'processed_path': '/content/drive/MyDrive/MLOps/data/processed.parquet',\n",
    "            'chunk_size': 100000\n",
    "        },\n",
    "        'model': {\n",
    "            'bert': {\n",
    "                'model_name': 'bert-base-uncased',\n",
    "                'num_labels': 2,\n",
    "                'epochs': 3,\n",
    "                'batch_size': 16,\n",
    "                'max_length': 128\n",
    "            }\n",
    "        },\n",
    "        'training': {\n",
    "            'output_dir': '/content/drive/MyDrive/MLOps/results',\n",
    "            'logging_dir': '/content/drive/MyDrive/MLOps/logs'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Sauvegarder la configuration\n",
    "    os.makedirs(os.path.dirname(config_path), exist_ok=True)\n",
    "    with open(config_path, 'w') as file:\n",
    "        yaml.safe_dump(config, file)\n",
    "\n",
    "    try:\n",
    "        logger.info(\"Début du prétraitement des données\")\n",
    "        data_loader = DataLoader(config_path)\n",
    "        data_loader.process_and_save_chunks()\n",
    "\n",
    "        logger.info(\"Début de l'entraînement BERT\")\n",
    "        bert_classifier = BertTextClassifier(config_path)\n",
    "        df = pd.read_parquet(data_loader.processed_path)  # Charger toutes les 100 000 lignes\n",
    "        bert_classifier.train(df)\n",
    "        bert_classifier.save_model('/content/drive/MyDrive/MLOps/models/bert_v1')\n",
    "        logger.info(\"Entraînement terminé\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur pendant l'exécution : {str(e)}\", exc_info=True)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a7eeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== IMPORTS ======================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, TrainerCallback\n",
    "import yaml\n",
    "import os\n",
    "import re\n",
    "import emoji\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# ====================== CONFIGURATION ======================\n",
    "# data_path = \"/content/new_dataset.csv\"  # <-- À adapter selon ton chemin\n",
    "config = {\n",
    "    'data': {\n",
    "        'raw_path': '/content/new_dataset.csv',\n",
    "        'processed_path': '/content/processed.csv',\n",
    "        'chunk_size': 10_000\n",
    "    },\n",
    "    'model': {\n",
    "        'bert': {\n",
    "            'model_name': 'bert-base-uncased',\n",
    "            'num_labels': 2,\n",
    "            'max_length': 128,\n",
    "            'epochs': 5,\n",
    "            'weight_decay': 0.01,\n",
    "            'batch_size': 16\n",
    "        }\n",
    "    }\n",
    "}\n",
    "os.makedirs('/content', exist_ok=True)\n",
    "with open('/content/config.yaml', 'w') as file:\n",
    "    yaml.safe_dump(config, file)\n",
    "\n",
    "# ====================== DATA LOADER ======================\n",
    "class DataLoader:\n",
    "    def __init__(self, config):\n",
    "        self.raw_path = config['data']['raw_path']\n",
    "        self.processed_path = config['data']['processed_path']\n",
    "        self.chunk_size = config['data']['chunk_size']\n",
    "\n",
    "    def preprocess_chunk(self, df_chunk):\n",
    "        df_chunk = df_chunk.dropna(subset=['text', 'label'])\n",
    "        df_chunk['text'] = df_chunk['text'].astype(str)\n",
    "        df_chunk['text'] = df_chunk['text'].apply(lambda x: re.sub(r'http\\S+|www\\S+|@\\w+|#\\w+', '', x))\n",
    "        df_chunk['text'] = df_chunk['text'].apply(lambda x: emoji.replace_emoji(x, replace=''))\n",
    "        df_chunk['text'] = df_chunk['text'].apply(lambda x: re.sub(r'[^\\w\\s!?]', '', x.lower()))\n",
    "        df_chunk = df_chunk[df_chunk['text'].str.len() > 10]\n",
    "        df_chunk = df_chunk[df_chunk['text'].str.split().str.len() > 2]\n",
    "        if df_chunk['label'].min() == 1:\n",
    "            df_chunk['label'] = df_chunk['label'] - 1\n",
    "        return df_chunk\n",
    "\n",
    "    def chunk_generator(self):\n",
    "        for chunk in pd.read_csv(self.raw_path, chunksize=self.chunk_size):\n",
    "            yield self.preprocess_chunk(chunk)\n",
    "\n",
    "# ====================== FEATURE ENGINEER ======================\n",
    "class FeatureEngineer:\n",
    "    def __init__(self, config_path):\n",
    "        self.config = yaml.safe_load(open(config_path, 'r'))\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.config['model']['bert']['model_name'])\n",
    "\n",
    "    def transform(self, texts, max_length=128):\n",
    "        return self.tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_token_type_ids=False,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "# ====================== DATASET ======================\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: v[idx] for k, v in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "# ====================== MODEL ======================\n",
    "global_metrics = []\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "class BertTrainer:\n",
    "    def __init__(self, config_path):\n",
    "        self.config_path = config_path\n",
    "        with open(config_path, 'r') as f:\n",
    "            self.config = yaml.safe_load(f)\n",
    "\n",
    "        self.model = BertForSequenceClassification.from_pretrained(\n",
    "            self.config['model']['bert']['model_name'],\n",
    "            num_labels=self.config['model']['bert']['num_labels']\n",
    "        )\n",
    "        self.feature_engineer = FeatureEngineer(config_path)\n",
    "        self.output_dir = \"/content/checkpoint\"\n",
    "\n",
    "    def train_on_chunk(self, df, chunk_num):\n",
    "        train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "            df['text'].tolist(),\n",
    "            df['label'].tolist(),\n",
    "            test_size=0.2,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        train_enc = self.feature_engineer.transform(train_texts)\n",
    "        val_enc = self.feature_engineer.transform(val_texts)\n",
    "\n",
    "        train_dataset = SentimentDataset(train_enc, train_labels)\n",
    "        val_dataset = SentimentDataset(val_enc, val_labels)\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.output_dir,\n",
    "            num_train_epochs=self.config['model']['bert']['epochs'],\n",
    "            per_device_train_batch_size=self.config['model']['bert']['batch_size'],\n",
    "            per_device_eval_batch_size=32,\n",
    "            weight_decay=self.config['model']['bert']['weight_decay'],\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"accuracy\",\n",
    "            logging_dir='./logs',\n",
    "            logging_steps=10,\n",
    "            save_total_limit=2,\n",
    "            resume_from_checkpoint=os.path.exists(os.path.join(self.output_dir, \"checkpoint-last\")),\n",
    "            overwrite_output_dir=False\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            # compute_metrics=lambda p: {'accuracy': (p.predictions.argmax(-1) == p.label_ids).mean()}\n",
    "            compute_metrics=compute_metrics\n",
    "        )\n",
    "\n",
    "        checkpoint_path = os.path.join(self.output_dir, \"checkpoint-last\") if os.path.exists(os.path.join(self.output_dir, \"checkpoint-last\")) else None\n",
    "        trainer.train(resume_from_checkpoint=checkpoint_path)\n",
    "\n",
    "        eval_results = trainer.evaluate()\n",
    "        eval_results[\"chunk\"] = chunk_num\n",
    "        global_metrics.append(eval_results)\n",
    "\n",
    "\n",
    "# ====================== MAIN ======================\n",
    "if __name__ == \"__main__\":\n",
    "    config_path = '/content/config.yaml'\n",
    "    data_loader = DataLoader(config)\n",
    "    trainer = BertTrainer(config_path)\n",
    "\n",
    "    for i, chunk_df in enumerate(data_loader.chunk_generator()):\n",
    "        print(f\"=== Entraînement sur le chunk {i+1} ===\")\n",
    "        trainer.train_on_chunk(chunk_df, chunk_num=i+1)\n",
    "\n",
    "    # Sauvegarde finale\n",
    "    trainer.model.save_pretrained(\"/content/models/final_bert\")\n",
    "    trainer.feature_engineer.tokenizer.save_pretrained(\"/content/models/final_bert\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e0eba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  code used to train model import sys\n",
    "import os\n",
    "import warnings\n",
    "import yaml\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.pytorch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from src.data.load_data import DataLoader\n",
    "from src.features.feature_engineering import FeatureEngineer\n",
    "from src.utils.helper_functions import timer_decorator\n",
    "\n",
    "# Configurer le logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Ignorer les avertissements spécifiques\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"_distutils_hack\")\n",
    "\n",
    "# Ajouter le chemin du projet au sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n",
    "\n",
    "\n",
    "# Définir les classes\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class TextClassificationModel:\n",
    "    def __init__(self, config_path: str):\n",
    "        with open(config_path, 'r') as file:\n",
    "            self.config = yaml.safe_load(file)\n",
    "        self.data_loader = DataLoader(config_path)\n",
    "    \n",
    "    def save_model(self, model, path: str):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def train(self, df: pd.DataFrame):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class RandomForestTextClassifier(TextClassificationModel):\n",
    "    def __init__(self, config_path: str):\n",
    "        super().__init__(config_path)\n",
    "        self.feature_engineer = FeatureEngineer(config_path)\n",
    "        self.model = SGDClassifier(loss='log_loss', max_iter=1000, tol=1e-3)\n",
    "\n",
    "    @timer_decorator\n",
    "    def train(self, df: pd.DataFrame):\n",
    "        logging.info(f\"Entraînement sur un lot de {len(df)} lignes\")\n",
    "        texts, labels = df['text'].tolist(), df['label'].tolist()\n",
    "        \n",
    "        # Vérifier les classes présentes\n",
    "        unique_labels = set(labels)\n",
    "        if len(unique_labels) < 2:\n",
    "            logging.warning(f\"Lot ignoré : contient seulement les labels {unique_labels}\")\n",
    "            return\n",
    "        \n",
    "        # Ajuster TF-IDF sur un échantillon si trop grand\n",
    "        sample_size = min(10000, len(texts))\n",
    "        self.feature_engineer.fit_tfidf(texts[:sample_size])\n",
    "        \n",
    "        # Calculer les poids des classes sur un échantillon\n",
    "        sample_labels = labels[:sample_size]\n",
    "        class_weights = compute_class_weight('balanced', classes=np.array([1, 2]), y=sample_labels)\n",
    "        class_weight_dict = {1: class_weights[0], 2: class_weights[1]}\n",
    "        \n",
    "        # Entraînement par lots\n",
    "        batch_size = 10000\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            batch_labels = np.array(labels[i:i + batch_size])\n",
    "            X_batch = self.feature_engineer.transform_tfidf(batch_texts)\n",
    "            sample_weights = np.array([class_weight_dict[label] for label in batch_labels])\n",
    "            self.model.partial_fit(X_batch, batch_labels, classes=np.array([1, 2]), sample_weight=sample_weights)\n",
    "            logging.info(f\"Lot {i//batch_size + 1} entraîné\")\n",
    "        \n",
    "        # Évaluation sur un sous-ensemble\n",
    "        eval_texts = texts[:1000]\n",
    "        eval_labels = labels[:1000]\n",
    "        X_eval = self.feature_engineer.transform_tfidf(eval_texts)\n",
    "        predictions = self.model.predict(X_eval)\n",
    "        accuracy = accuracy_score(eval_labels, predictions)\n",
    "        logging.info(f\"Précision sur le sous-ensemble d'évaluation : {accuracy:.4f}\")\n",
    "        \n",
    "        # Créer ou récupérer l'expérience MLflow\n",
    "        experiment_name = \"TextClassificationExperiment\"\n",
    "        experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "        if experiment is None:\n",
    "            experiment_id = mlflow.create_experiment(experiment_name)\n",
    "        else:\n",
    "            experiment_id = experiment.experiment_id\n",
    "        \n",
    "        with mlflow.start_run(experiment_id=experiment_id):\n",
    "            mlflow.log_param(\"model_type\", \"SGDClassifier\")\n",
    "            mlflow.log_param(\"batch_size\", batch_size)\n",
    "            mlflow.log_metric(\"accuracy\", accuracy)\n",
    "            mlflow.sklearn.log_model(self.model, \"sgd_model\")\n",
    "            logging.info(\"Métriques et modèle loggés dans MLflow\")\n",
    "\n",
    "    def save_model(self, path):\n",
    "        import pickle\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump({'model': self.model, 'vectorizer': self.feature_engineer.tfidf}, f)\n",
    "        logging.info(f\"Modèle sauvegardé à {path}\")\n",
    "\n",
    "class BertTextClassifier(TextClassificationModel):\n",
    "    def __init__(self, config_path: str):\n",
    "        super().__init__(config_path)\n",
    "        self.model_name = self.config['model']['bert']['model_name']\n",
    "        self.num_labels = self.config['model']['bert']['num_labels']\n",
    "        self.max_length = self.config['model']['bert']['max_length']\n",
    "        self.feature_engineer = FeatureEngineer(config_path)  # Utilisation de FeatureEngineer\n",
    "        self.model = BertForSequenceClassification.from_pretrained(\n",
    "            self.model_name,\n",
    "            num_labels=self.num_labels\n",
    "        )\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        logging.info(f\"Modèle BERT chargé ({self.model_name}) sur {self.device}\")\n",
    "\n",
    "    def train(self, df):\n",
    "        logging.info(f\"Entraînement BERT sur {len(df)} lignes\")\n",
    "        # Ajuster les labels si nécessaire\n",
    "        labels = df['label'].values\n",
    "        if labels.min() == 1:\n",
    "            logging.info(\"Ajustement des labels de 1,2 à 0,1\")\n",
    "            labels = labels - 1\n",
    "        train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "            df['text'].values, labels, test_size=0.2, random_state=42\n",
    "        )\n",
    "        # Utiliser FeatureEngineer pour la tokenisation\n",
    "        train_encodings = self.feature_engineer.transform_bert(train_texts.tolist(), max_length=self.max_length)\n",
    "        val_encodings = self.feature_engineer.transform_bert(val_texts.tolist(), max_length=self.max_length)\n",
    "        train_dataset = SentimentDataset(train_texts, train_labels, self.feature_engineer.bert_tokenizer, self.max_length)\n",
    "        val_dataset = SentimentDataset(val_texts, val_labels, self.feature_engineer.bert_tokenizer, self.max_length)\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir='/results',\n",
    "            num_train_epochs=self.config['model']['bert']['epochs'],\n",
    "            per_device_train_batch_size=self.config['model']['bert']['batch_size'],\n",
    "            per_device_eval_batch_size=self.config['model']['bert']['batch_size'],\n",
    "            warmup_steps=500,\n",
    "            weight_decay=0.01,\n",
    "            logging_dir='/logs',\n",
    "            logging_steps=10,\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True,\n",
    "            dataloader_pin_memory=torch.cuda.is_available(),\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            compute_metrics=lambda p: {'accuracy': (p.predictions.argmax(-1) == p.label_ids).mean()}\n",
    "        )\n",
    "\n",
    "        # MLflow\n",
    "        experiment_name = \"TextClassificationExperiment\"\n",
    "        experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "        if experiment is None:\n",
    "            experiment_id = mlflow.create_experiment(experiment_name)\n",
    "        else:\n",
    "            experiment_id = experiment.experiment_id\n",
    "\n",
    "        with mlflow.start_run(experiment_id=experiment_id):\n",
    "            mlflow.log_param(\"model_type\", \"BERT\")\n",
    "            mlflow.log_param(\"model_name\", self.model_name)\n",
    "            mlflow.log_param(\"epochs\", self.config['model']['bert']['epochs'])\n",
    "            mlflow.log_param(\"batch_size\", self.config['model']['bert']['batch_size'])\n",
    "            mlflow.log_param(\"max_length\", self.max_length)\n",
    "\n",
    "            # Entraînement\n",
    "            trainer.train()\n",
    "\n",
    "            # Évaluation\n",
    "            predictions = trainer.predict(val_dataset).predictions.argmax(-1)\n",
    "            accuracy = accuracy_score(val_labels, predictions)\n",
    "            mlflow.log_metric(\"val_accuracy\", accuracy)\n",
    "            mlflow.pytorch.log_model(self.model, \"bert_model\")\n",
    "            logging.info(f\"Précision sur validation : {accuracy:.4f}\")\n",
    "\n",
    "    def save_model(self, output_dir):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        self.model.save_pretrained(output_dir)\n",
    "        self.feature_engineer.bert_tokenizer.save_pretrained(output_dir)\n",
    "        logging.info(f\"Modèle BERT sauvegardé à {output_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config_path = os.path.join(os.path.dirname(__file__), \"../../config/config.yaml\")\n",
    "    data_loader = DataLoader(config_path)\n",
    "    \n",
    "    try:\n",
    "        # logging.info(\"Début du prétraitement des données\")\n",
    "        # data_loader.process_and_save_chunks()\n",
    "        \n",
    "        # logging.info(\"Début de l'entraînement Random Forest (SGDClassifier)\")\n",
    "        # rf_classifier = RandomForestTextClassifier(config_path)\n",
    "        # for texts, labels in data_loader.data_generator(batch_size=10000):\n",
    "        #     df_chunk = pd.DataFrame({'text': texts, 'label': labels})\n",
    "        #     rf_classifier.train(df_chunk)\n",
    "        \n",
    "        # rf_classifier.save_model(\"models/random_forest_v1.pkl\")\n",
    "        \n",
    "        logging.info(\"Début de l'entraînement BERT\")\n",
    "        bert_classifier = BertTextClassifier(config_path)\n",
    "        for texts, labels in data_loader.data_generator(batch_size=10000):\n",
    "            df_chunk = pd.DataFrame({'text': texts, 'label': labels})\n",
    "            bert_classifier.train(df_chunk)\n",
    "    \n",
    "        bert_classifier.save_model(\"models/bert_v1\")\n",
    "        logging.info(\"Entraînement terminé\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Erreur pendant l'exécution : {str(e)}\", exc_info=True)\n",
    "        raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
