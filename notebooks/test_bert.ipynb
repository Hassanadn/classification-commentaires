{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c6db441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 5000 lignes aléatoires écrites dans ../data/raw/new_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "\n",
    "def reservoir_sample(file_path, sample_size):\n",
    "    \"\"\"Sélectionne aléatoirement sample_size lignes d'un gros fichier CSV.\"\"\"\n",
    "    reservoir = []\n",
    "    with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        header = next(reader)  # Sauvegarder l'en-tête\n",
    "        for i, row in enumerate(reader):\n",
    "            if i < sample_size:\n",
    "                reservoir.append(row)\n",
    "            else:\n",
    "                j = random.randint(0, i)\n",
    "                if j < sample_size:\n",
    "                    reservoir[j] = row\n",
    "    return header, reservoir\n",
    "\n",
    "# Paramètres\n",
    "input_file = '../data/raw/train.csv'\n",
    "output_file = '../data/raw/new_dataset.csv'\n",
    "sample_size = 5_000\n",
    "\n",
    "# Exécution\n",
    "header, sample_rows = reservoir_sample(input_file, sample_size)\n",
    "\n",
    "# Écriture dans le nouveau fichier\n",
    "with open(output_file, mode='w', newline='', encoding='utf-8') as f_out:\n",
    "    writer = csv.writer(f_out)\n",
    "    writer.writerow(header)\n",
    "    writer.writerows(sample_rows)\n",
    "\n",
    "print(f\"✅ {sample_size} lignes aléatoires écrites dans {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e0eba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  code used to train model import sys\n",
    "import os\n",
    "import warnings\n",
    "import yaml\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.pytorch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from src.data.load_data import DataLoader\n",
    "from src.features.feature_engineering import FeatureEngineer\n",
    "from src.utils.helper_functions import timer_decorator\n",
    "\n",
    "# Configurer le logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Ignorer les avertissements spécifiques\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"_distutils_hack\")\n",
    "\n",
    "# Ajouter le chemin du projet au sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n",
    "\n",
    "\n",
    "# Définir les classes\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class TextClassificationModel:\n",
    "    def __init__(self, config_path: str):\n",
    "        with open(config_path, 'r') as file:\n",
    "            self.config = yaml.safe_load(file)\n",
    "        self.data_loader = DataLoader(config_path)\n",
    "    \n",
    "    def save_model(self, model, path: str):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def train(self, df: pd.DataFrame):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class RandomForestTextClassifier(TextClassificationModel):\n",
    "    def __init__(self, config_path: str):\n",
    "        super().__init__(config_path)\n",
    "        self.feature_engineer = FeatureEngineer(config_path)\n",
    "        self.model = SGDClassifier(loss='log_loss', max_iter=1000, tol=1e-3)\n",
    "\n",
    "    @timer_decorator\n",
    "    def train(self, df: pd.DataFrame):\n",
    "        logging.info(f\"Entraînement sur un lot de {len(df)} lignes\")\n",
    "        texts, labels = df['text'].tolist(), df['label'].tolist()\n",
    "        \n",
    "        # Vérifier les classes présentes\n",
    "        unique_labels = set(labels)\n",
    "        if len(unique_labels) < 2:\n",
    "            logging.warning(f\"Lot ignoré : contient seulement les labels {unique_labels}\")\n",
    "            return\n",
    "        \n",
    "        # Ajuster TF-IDF sur un échantillon si trop grand\n",
    "        sample_size = min(10000, len(texts))\n",
    "        self.feature_engineer.fit_tfidf(texts[:sample_size])\n",
    "        \n",
    "        # Calculer les poids des classes sur un échantillon\n",
    "        sample_labels = labels[:sample_size]\n",
    "        class_weights = compute_class_weight('balanced', classes=np.array([1, 2]), y=sample_labels)\n",
    "        class_weight_dict = {1: class_weights[0], 2: class_weights[1]}\n",
    "        \n",
    "        # Entraînement par lots\n",
    "        batch_size = 10000\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            batch_labels = np.array(labels[i:i + batch_size])\n",
    "            X_batch = self.feature_engineer.transform_tfidf(batch_texts)\n",
    "            sample_weights = np.array([class_weight_dict[label] for label in batch_labels])\n",
    "            self.model.partial_fit(X_batch, batch_labels, classes=np.array([1, 2]), sample_weight=sample_weights)\n",
    "            logging.info(f\"Lot {i//batch_size + 1} entraîné\")\n",
    "        \n",
    "        # Évaluation sur un sous-ensemble\n",
    "        eval_texts = texts[:1000]\n",
    "        eval_labels = labels[:1000]\n",
    "        X_eval = self.feature_engineer.transform_tfidf(eval_texts)\n",
    "        predictions = self.model.predict(X_eval)\n",
    "        accuracy = accuracy_score(eval_labels, predictions)\n",
    "        logging.info(f\"Précision sur le sous-ensemble d'évaluation : {accuracy:.4f}\")\n",
    "        \n",
    "        # Créer ou récupérer l'expérience MLflow\n",
    "        experiment_name = \"TextClassificationExperiment\"\n",
    "        experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "        if experiment is None:\n",
    "            experiment_id = mlflow.create_experiment(experiment_name)\n",
    "        else:\n",
    "            experiment_id = experiment.experiment_id\n",
    "        \n",
    "        with mlflow.start_run(experiment_id=experiment_id):\n",
    "            mlflow.log_param(\"model_type\", \"SGDClassifier\")\n",
    "            mlflow.log_param(\"batch_size\", batch_size)\n",
    "            mlflow.log_metric(\"accuracy\", accuracy)\n",
    "            mlflow.sklearn.log_model(self.model, \"sgd_model\")\n",
    "            logging.info(\"Métriques et modèle loggés dans MLflow\")\n",
    "\n",
    "    def save_model(self, path):\n",
    "        import pickle\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump({'model': self.model, 'vectorizer': self.feature_engineer.tfidf}, f)\n",
    "        logging.info(f\"Modèle sauvegardé à {path}\")\n",
    "\n",
    "class BertTextClassifier(TextClassificationModel):\n",
    "    def __init__(self, config_path: str):\n",
    "        super().__init__(config_path)\n",
    "        self.model_name = self.config['model']['bert']['model_name']\n",
    "        self.num_labels = self.config['model']['bert']['num_labels']\n",
    "        self.max_length = self.config['model']['bert']['max_length']\n",
    "        self.feature_engineer = FeatureEngineer(config_path)  # Utilisation de FeatureEngineer\n",
    "        self.model = BertForSequenceClassification.from_pretrained(\n",
    "            self.model_name,\n",
    "            num_labels=self.num_labels\n",
    "        )\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        logging.info(f\"Modèle BERT chargé ({self.model_name}) sur {self.device}\")\n",
    "\n",
    "    def train(self, df):\n",
    "        logging.info(f\"Entraînement BERT sur {len(df)} lignes\")\n",
    "        # Ajuster les labels si nécessaire\n",
    "        labels = df['label'].values\n",
    "        if labels.min() == 1:\n",
    "            logging.info(\"Ajustement des labels de 1,2 à 0,1\")\n",
    "            labels = labels - 1\n",
    "        train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "            df['text'].values, labels, test_size=0.2, random_state=42\n",
    "        )\n",
    "        # Utiliser FeatureEngineer pour la tokenisation\n",
    "        train_encodings = self.feature_engineer.transform_bert(train_texts.tolist(), max_length=self.max_length)\n",
    "        val_encodings = self.feature_engineer.transform_bert(val_texts.tolist(), max_length=self.max_length)\n",
    "        train_dataset = SentimentDataset(train_texts, train_labels, self.feature_engineer.bert_tokenizer, self.max_length)\n",
    "        val_dataset = SentimentDataset(val_texts, val_labels, self.feature_engineer.bert_tokenizer, self.max_length)\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir='/results',\n",
    "            num_train_epochs=self.config['model']['bert']['epochs'],\n",
    "            per_device_train_batch_size=self.config['model']['bert']['batch_size'],\n",
    "            per_device_eval_batch_size=self.config['model']['bert']['batch_size'],\n",
    "            warmup_steps=500,\n",
    "            weight_decay=0.01,\n",
    "            logging_dir='/logs',\n",
    "            logging_steps=10,\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True,\n",
    "            dataloader_pin_memory=torch.cuda.is_available(),\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            compute_metrics=lambda p: {'accuracy': (p.predictions.argmax(-1) == p.label_ids).mean()}\n",
    "        )\n",
    "\n",
    "        # MLflow\n",
    "        experiment_name = \"TextClassificationExperiment\"\n",
    "        experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "        if experiment is None:\n",
    "            experiment_id = mlflow.create_experiment(experiment_name)\n",
    "        else:\n",
    "            experiment_id = experiment.experiment_id\n",
    "\n",
    "        with mlflow.start_run(experiment_id=experiment_id):\n",
    "            mlflow.log_param(\"model_type\", \"BERT\")\n",
    "            mlflow.log_param(\"model_name\", self.model_name)\n",
    "            mlflow.log_param(\"epochs\", self.config['model']['bert']['epochs'])\n",
    "            mlflow.log_param(\"batch_size\", self.config['model']['bert']['batch_size'])\n",
    "            mlflow.log_param(\"max_length\", self.max_length)\n",
    "\n",
    "            # Entraînement\n",
    "            trainer.train()\n",
    "\n",
    "            # Évaluation\n",
    "            predictions = trainer.predict(val_dataset).predictions.argmax(-1)\n",
    "            accuracy = accuracy_score(val_labels, predictions)\n",
    "            mlflow.log_metric(\"val_accuracy\", accuracy)\n",
    "            mlflow.pytorch.log_model(self.model, \"bert_model\")\n",
    "            logging.info(f\"Précision sur validation : {accuracy:.4f}\")\n",
    "\n",
    "    def save_model(self, output_dir):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        self.model.save_pretrained(output_dir)\n",
    "        self.feature_engineer.bert_tokenizer.save_pretrained(output_dir)\n",
    "        logging.info(f\"Modèle BERT sauvegardé à {output_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config_path = os.path.join(os.path.dirname(__file__), \"../../config/config.yaml\")\n",
    "    data_loader = DataLoader(config_path)\n",
    "    \n",
    "    try:\n",
    "        # logging.info(\"Début du prétraitement des données\")\n",
    "        # data_loader.process_and_save_chunks()\n",
    "        \n",
    "        # logging.info(\"Début de l'entraînement Random Forest (SGDClassifier)\")\n",
    "        # rf_classifier = RandomForestTextClassifier(config_path)\n",
    "        # for texts, labels in data_loader.data_generator(batch_size=10000):\n",
    "        #     df_chunk = pd.DataFrame({'text': texts, 'label': labels})\n",
    "        #     rf_classifier.train(df_chunk)\n",
    "        \n",
    "        # rf_classifier.save_model(\"models/random_forest_v1.pkl\")\n",
    "        \n",
    "        logging.info(\"Début de l'entraînement BERT\")\n",
    "        bert_classifier = BertTextClassifier(config_path)\n",
    "        for texts, labels in data_loader.data_generator(batch_size=10000):\n",
    "            df_chunk = pd.DataFrame({'text': texts, 'label': labels})\n",
    "            bert_classifier.train(df_chunk)\n",
    "    \n",
    "        bert_classifier.save_model(\"models/bert_v1\")\n",
    "        logging.info(\"Entraînement terminé\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Erreur pendant l'exécution : {str(e)}\", exc_info=True)\n",
    "        raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
